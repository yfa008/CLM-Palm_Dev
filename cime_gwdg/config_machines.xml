<?xml version="1.0"?>
<!-- This is an ordered list, not all fields are required, optional fields are noted below. -->
<config_machines version="2.0">
  <!-- MACH is the name that you will use in machine options -->
  <machine MACH="gwdg">
    <!-- DESC: a text description of the machine, this field is current not used in code-->
    <DESC>GWDG HPC cluster, GÃ¶ttingen, os is Linux, xx pes/node, batch system is SLURM</DESC>
    <!-- NODENAME_REGEX: a regular expression used to identify this machine
    it must work on compute nodes as well as login nodes, use machine option
    to create_test or create_newcase if this flag is not available -->
    <NODENAME_REGEX>.*\.global\.gwdg\.cluster</NODENAME_REGEX>
    <!-- OS: the operating system of this machine. Passed to cppflags for
    compiled programs as -DVALUE  recognized are LINUX, AIX, Darwin, CNL -->
    <OS>LINUX</OS>
    <!-- PROXY: optional http proxy for access to the internet-->
    <PROXY> https://howto.get.out </PROXY>
    <!-- COMPILERS: compilers supported on this machine, comma seperated list, first is default -->
    <COMPILERS>intel,gnu</COMPILERS>
    <!-- MPILIBS: mpilibs supported on this machine, comma seperated list,
    first is default, mpi-serial is assumed and not required in this list-->
    <MPILIBS>impi,openmpi</MPILIBS>
    <!-- PROJECT: A project or account number used for batch jobs
    can be overridden in environment or $HOME/.cime/config -->
    <PROJECT>couldbethis</PROJECT>
    <!-- SAVE_TIMING_DIR: (Acme only) directory to write timing output to -->
    <SAVE_TIMING_DIR> </SAVE_TIMING_DIR>
    <!-- CIME_OUTPUT_ROOT: Base directory for case output,
    the case/bld and case/run directories are written below here -->
    <CIME_OUTPUT_ROOT>/usr/users/$USER/cesm/exeroot</CIME_OUTPUT_ROOT>
    <!-- DIN_LOC_ROOT: location of the inputdata data directory
    inputdata is downloaded automatically on a case by case basis as
    long as the user has write access to this directory.   We recommend that
    all cime model users on a system share an inputdata directory
    as it can be quite large -->
    <DIN_LOC_ROOT>/usr/users/model4bk/CESM/INPUT</DIN_LOC_ROOT>
    <!-- DIN_LOC_ROOT_CLMFORC: override of DIN_LOC_ROOT specific to CLM
    forcing data -->
    <DIN_LOC_ROOT_CLMFORC>/usr/users/model4bk/CESM/INPUT/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <!-- DOUT_S_ROOT: root directory of short term archive files, short term
    archiving moves model output data out of the run directory, but
    keeps it on disk-->
    <DOUT_S_ROOT>/usr/users/$USER/cesm/archive/$CASE</DOUT_S_ROOT>
    <!-- BASELINE_ROOT:  Root directory for system test baseline files -->
    <BASELINE_ROOT>/usr/users/$USER/cesm/cesm_baselines</BASELINE_ROOT>
    <!-- CCSM_CPRNC: location of the cprnc tool, compares model output in testing-->
    <CCSM_CPRNC>/usr/users/$USER/cesm/ctsm/cime/tools/cprnc/</CCSM_CPRNC>
    <!-- GMAKE: gnu compatible make tool, default is 'gmake' -->
    <GMAKE>gmake</GMAKE>
    <!-- GMAKE_J: optional number of threads to pass to the gmake flag -->
    <GMAKE_J>8</GMAKE_J>
    <!-- BATCH_SYSTEM: batch system used on this machine,
    supported values are: none, cobalt, lsf, pbs, slurm -->
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <!-- SUPPORTED_BY: contact information for support for this system
    this field is not used in code -->
    <SUPPORTED_BY>fmoyano -at- uni-goettingen.de</SUPPORTED_BY>
    <!-- MAX_TASKS_PER_NODE: maximum number of threads*tasks per
    shared memory node on this machine,
    should always be >= MAX_MPITASKS_PER_NODE -->
    <MAX_TASKS_PER_NODE>24</MAX_TASKS_PER_NODE>
    <!-- MAX_MPITASKS_PER_NODE: number of physical PES per shared node on
    this machine, in practice the MPI tasks per node will not exceed this value -->
    <MAX_MPITASKS_PER_NODE>24</MAX_MPITASKS_PER_NODE>
    <!-- PROJECT_REQUIRED: Does this machine require a project to be specified to
    the batch system?  See PROJECT above -->
    <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
    <!-- mpirun: The mpi exec to start a job on this machine, supported values
    are values listed in MPILIBS above, default and mpi-serial -->
    <mpirun mpilib="default">
      <!-- name of the exectuable used to launch mpi jobs -->
      <executable>mpiexec</executable>
      <!-- arguments to the mpiexec command, the name attribute here is ignored-->
      <arguments>
        <arg name="ntasks"> -np {{ total_tasks }} </arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="openmpi">
      <executable>mpiexec</executable>
    </mpirun>
    <!-- module system: allowed module_system type values are:
    module  http://www.tacc.utexas.edu/tacc-projects/mclay/lmod
    soft http://www.mcs.anl.gov/hs/software/systems/softenv/softenv-intro.html
    none
  -->
   <module_system type="module">         
    <init_path lang="sh">/opt/sw/etc/lmod/profile</init_path>    
    <cmd_path lang="sh">module -q</cmd_path> 
    <cmd_path lang="python">module -q</cmd_path>    
    <modules>
      <command name="purge"/>
      <command name="load">subversion</command>
    </modules>
    <modules compiler="intel">            
      <command name="load">intel-oneapi-compilers</command>  
      <command name="load">intel-oneapi-mpi</command>
      <command name="load">intel-oneapi-mkl</command>     
      <command name="load">netcdf-c</command>
      <command name="load">netcdf-fortran</command>
      <command name="load">hdf5</command>
    </modules>
    <modules compiler="gnu">
      <command name="load">gcc</command>
      <command name="load">netcdf-c</command>
      <command name="load">netcdf-fortran</command>
      <command name="load">openblas</command>      
    </modules>
    <modules mpilib="impi">
      <command name="load">intel-mpi</command>
      <command name="load">hdf5</command>
      <command name="load">netcdf-c</command>
      <command name="load">netcdf-fortran</command>
    </modules>
    <modules mpilib="openmpi">
      <command name="load">openmpi</command>
      <command name="load">netcdf-c</command>
      <command name="load">netcdf-fortran</command>
    </modules>
  </module_system>
 <!-- old setting
  <module_system type="module">
    <init_path lang="perl">/cm/local/apps/environment-modules/3.2.10/init/perl.pm</init_path>
    <init_path lang="python">/cm/local/apps/environment-modules/3.2.10/init/python.py</init_path>
    <init_path lang="sh">/cm/local/apps/environment-modules/3.2.10/init/sh</init_path>
    <init_path lang="csh">/cm/local/apps/environment-modules/3.2.10/init/csh</init_path>
    <cmd_path lang="perl">/cm/local/apps/environment-modules/3.2.10/bin/modulecmd perl</cmd_path>
    <cmd_path lang="python">/cm/local/apps/environment-modules/3.2.10/bin/modulecmd python</cmd_path>
    <cmd_path lang="sh">module</cmd_path>
    <cmd_path lang="csh">module</cmd_path>
    <modules>
      <command name="purge"/>
      <command name="load">subversion</command>
    </modules>
    <modules compiler="intel">
      <command name="load">intel/compiler</command>
      <command name="load">intel/mkl</command>
      <command name="load">netcdf/intel/64</command>
    </modules>
    <modules compiler="gnu">
      <command name="load">gcc</command>
      <command name="load">netcdf/gcc</command>
      <command name="load">blas/gcc</command>
      <command name="load">lapack/gcc</command>
    </modules>
    <modules mpilib="impi">
      <command name="load">intel/mpi</command>
      <command name="load">hdf5/parallel/intelmpi/1.8.11</command>
      <command name="load">netcdf/intel/parallel</command>
    </modules>
    <modules mpilib="openmpi">
      <command name="load">openmpi/gcc</command>
      <command name="load">netcdf/gcc</command>
    </modules>
  </module_system>
 -->
  <!-- environment variables, a blank entry will unset a variable -->
  <environment_variables>
    <env name="OMP_STACKSIZE">64M</env>
    <env name="MPI_TYPE_DEPTH">16</env>
  </environment_variables>
  <!-- resource settings as defined in https://docs.python.org/2/library/resource.html -->
  <resource_limits>
    <resource name="RLIMIT_STACK">-1</resource>
  </resource_limits>
</machine>
</config_machines>
